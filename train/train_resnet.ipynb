{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n",
      "Device name: NVIDIA GeForce RTX 2080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# pyton 3.9.11 / cuda / windows\n",
    "# pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121 \n",
    "# ipython kernel install --name \"cuda-train\" --user\n",
    "# pip install ipython matplotlib\n",
    "# print(\"Cuda available: \", torch.cuda.is_available())\n",
    "# print(\"Device name:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/severin/Documents/GitHub/u-net-segmentation-of-streets-and-cars/train/cityscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    return torch.from_numpy(np.array(img)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data transformation (you might need to customize these)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "def transform_image(img):\n",
    "    return torch.from_numpy(np.array(img)).long()\n",
    "\n",
    "# Define your target transformation\n",
    "target_transforms = transforms.Compose([\n",
    " transforms.Resize((256, 256)), # Resize target\n",
    " transforms.Lambda(lambda img: torch.Tensor(np.array(img))), \n",
    "])\n",
    "\n",
    "# Create Cityscapes dataset instance\n",
    "dataset = Cityscapes(root=data_path, split='train', mode='fine', \n",
    "                     target_type='semantic', transform=data_transforms, \n",
    "                     target_transform=target_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first image and its segmentation mask\n",
    "img, smnt = dataset[0]\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "img_np = img.permute(1, 2, 0).numpy()\n",
    "smnt_np = np.array(smnt)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display the image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Image')\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the segmentation mask\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Segmentation Mask')\n",
    "plt.imshow(smnt_np)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for easy iteration\n",
    "batch_size = 32 \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "\n",
    "# Load a pretrained model\n",
    "model = models.segmentation.fcn_resnet101(weights='DEFAULT') # Load a pretrained FCN-ResNet101 \n",
    "\n",
    "# Replace the classifier part for your task\n",
    "num_classes = len(Cityscapes.classes)\n",
    "model.classifier[-1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1)) \n",
    "\n",
    "# Load previously saved model weights (for example, epoch 1)\n",
    "checkpoint = torch.load('model_weights.pth')  # Change the file path to your desired checkpoint\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in training mode\n",
    "model.train()\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/∞\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = None\n",
    "epoch = 0\n",
    "\n",
    "while True:\n",
    "    epoch += 1\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs if num_epochs else \"∞\"))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    batch_count = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        batch_count += 1\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).long()  # Convert the type of labels to Long\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs['out']\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        print('Batch {}/{} - Loss: {:.4f}'.format(batch_count, total_batches, batch_loss))\n",
    "\n",
    "        del loss, outputs, labels  # Manually delete tensors to free memory\n",
    "\n",
    "    average_loss = total_loss / total_batches\n",
    "\n",
    "    print('Epoch completed. Average Loss: {:.4f}'.format(average_loss))\n",
    "\n",
    "    # Save model weights after each epoch\n",
    "    torch.save(model.state_dict(), f'model_weights_epoch_{epoch}.pth')\n",
    "\n",
    "    if num_epochs and epoch >= num_epochs:\n",
    "        break\n",
    "\n",
    "print('Training completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
